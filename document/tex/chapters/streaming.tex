% !TEX root = ../../document.tex

\documentclass{subfiles}

\begin{document}

  \chapter{Algoritmos para Streaming}
  \label{chap:streaming}


    \section{Introducción}
    \label{sec:streaming_intro}

      \paragraph{}
      Los \emph{algoritmos para streaming} son una estrategia de diseño algorítmica basada en el procesamiento secuencial de la entrada, lo cual encaja en marcos en los cuales los datos tienen una componente dinámica. Además, este contexto se amolda perfectamente en los casos en que el tamaño de los mismos es tan elevado que no es posible mantenerlos de manera completa en la memoria del sistema. Dicha problemática es precisamente la que surge con el denominado \emph{Big Data}, que al trabajar con conjuntos masivos de datos en el orden de gigabytes, terabytes o incluso petabytes, no se pueden procesar utilizando estrategias clásicas que presuponen que se dispone de todos los datos de manera directa e inmediata.

      \paragraph{}
      Por tanto, en dicho contexto se presupone un espacio de almacenamiento en disco de tamaño infinito, mientras que se restringe el espacio de trabajo o memoria a un tamaño limitado y mucho menor que el del conjunto de datos con el que se trabaja. Mediante estas presuposiciones fijadas a priori cobra especial importancia el diseño de algoritmos en el modelo en streaming, que tratan de reducir el número de peticiones al espacio de almacenamiento o disco, lo cual genera una gran reducción en el tiempo de procesamiento.

      \paragraph{}
      Además, bajo este modelo es trivial realizar una extensión de los algoritmos y técnicas para su uso en entornos dinámicos en los cuales el conjunto de datos varía con respecto del tiempo, añadiendo y eliminando nuevos datos. Debido a estas características la investigación en el campo de los \emph{algoritmos para streaming} a cobrado una gran importancia. En este capítulo se pretende realizar una introducción conceptual acerca de los mismos, además de realizar una exposición acerca de los algoritmos más relevantes en dicha área.

      \subsection{Computación en Tiempo Real}
      \label{sec:realtime_computing}
        \paragraph{}
        El primer concepto del que hablaremos es \textbf{Computación en Tiempo Real}, que tal y cómo describen Shin y Ramanathan \cite{259423} se carácteriza por tres términos que se exponen a continuación:

        \begin{itemize}

          \item \textbf{Tiempo}\emph{(time)}: En la disciplina de \emph{Computación en Tiempo Real} el tiempo de ejecución de una determinada tarea es especialmente crucial para garantizar el correcto desarrollo del cómputo, debido a que se asume un plazo de ejecución permitido, a partir del cual la solución del problema deja de tener validez. Shin y Ramanathan\cite{259423} diferencian entre tres categorías dentro de dicha restricción, a las cuales denominan \emph{hard}, \emph{firm} y \emph{soft}, dependiendo del grado de relajación de la misma.

          \item \textbf{Confiabilidad}\emph{(correctness)}: Otro de los puntos cruciales en un sistema de \emph{Computación en Tiempo Real} es la determinación de una unidad de medida o indicador acerca de las garantías de una determinada solución algorítmica para cumplir lo que promete de manera correcta en el tiempo esperado.

          \item \textbf{Entorno}\emph{(environment)}: El último factor que indican Shin y Ramanathan\cite{259423} para describir un sistema de \emph{Computación en Tiempo Real} es el entorno del mismo, debido a que este condiciona el conjunto de tareas y la periodicidad en que se deben llevar a cabo. Por esta razón, realizan una diferenciación entre:
          \begin{enumerate*}[label=\itshape\alph*\upshape)]
  					\item tareas periódicas \emph{periodic tasks} las cuales se realizan secuencialmente a partir de la finalización de una ventana de tiempo, y
  					\item tareas no periódicas \emph{periodic tasks} que se llevan a cabo debido al suceso de un determinado evento externo.
  				\end{enumerate*}

        \end{itemize}


      \subsection{Problemas Dinámicos}
      \label{sec:dynamic_problems}

        \paragraph{}
        Una vez completada la descripción acerca de lo que se puede definir como \emph{Computación en Tiempo Real}, conviene realizar una descripción desde el punto de vista de la \emph{teoría de complejidad computacional}. Para definir este tipo de problemas, se utiliza el término \emph{problemas dinámicos}, los cuales consisten en aquellos en los cuales es necesario recalcular su solución conforme el tiempo avanza debido a variaciones en los parámetros de entrada del problema (Nótese que dicho término no debe confundirse con la estrategia de \emph{programación dinámica} para el diseño de algoritmos).

        \paragraph{}
        Existen distintas vertientes dependiendo del punto de vista desde el que se estudien, tanto de la naturaleza del problema (soluciones dependientes temporalmente unas de otras o soluciones aisladas) como de los parámetros de entrada (entrada completa en cada nueva ejecución o variación respecto de la anterior). Los \emph{Algoritmos para Streaming} están diseñados para resolver \emph{problemas dinámicos}, por lo que en la sección \ref{sec:streaming_model}, se describe en profundidad el modelo en que se enmarcan.

        \paragraph{}
        A continuación se indican los principales indicadores utilizados para describir la complejidad de una determinada solución algorítmica destinada a resolver un problema de dicha naturaleza:

        \begin{itemize}
          \item Espacio: Cantidad de espacio utilizado en memoria durante la ejecución del algoritmo.
          \item Inicialización: Tiempo necesario para la inicialización del algoritmo.
          \item Procesado: Tiempo necesario para procesar una determinada entrada.
          \item Consulta: Tiempo necesario para procesar la solución a partir de los datos de entrada procesados hasta el momento.
        \end{itemize}


      \subsection{Algoritmos Online vs Algoritmos Offline}

        \paragraph{}
        Una vez descrita la problemática de \emph{Computación en Tiempo Real} en la sección \ref{sec:realtime_computing} y la categoría de \emph{Problemas Dinámicos} en la sección \ref{sec:dynamic_problems}, en esta sección se pretende ilustrar la diferencia entre los \emph{Algoritmos Online} y los \emph{Algoritmos Offline}. Para ello, se ha seguido la diferenciación propuesta por Karp \cite{Karp:1992:OAV:645569.659725}, en la cual se plantea el problema de la siguiente manera (Se utilizará la misma notación que sigue Muthukrishnan\cite{Muthukrishnan:2005:DSA:1166409.1166410} para tratar mantener la consistencia durante todo el documento): Sea $A$ el conjunto de datos o eventos de entrada, siendo cada $A[i]$ el elemento \emph{$i$-ésimo} del conjunto, y que en el caso de los \emph{Algoritmos Online} supondremos que es el elemento recibido en el instante \emph{i}. A continuación se muestran las características de cada subgrupo:

        \begin{itemize}
          \item \textbf{Algoritmos Offline}: Esta categoría contiene todos los algoritmos que realizan el cómputo suponiendo el acceso a cualquier elemento del conjunto de datos $A$ durante cualquier momento de su ejecución. Además, en esta categoría se impone la restricción de que el $A$ debe ser invariante respecto del tiempo, lo que conlleva que para la adaptación del resultado a cambios en la entrada, este tenga que realizar una nueva ejecución desde su estado inicial. Nótese por tanto, que dentro de este grupo se engloba la mayoría de algoritmos utilizados comúnmente.

          \item \textbf{Algoritmos Online}: Son aquellos que calculan el resultado a partir de una secuencia de sucesos $A[i]$, los cuales generan un resultado dependiente de la entrada actual, y posiblemente de las anteriores. A partir de dicha estrategia, se añade una componente dinámica, la cual permite que el tamaño del conjunto de datos de entrada $A$ no tenga impuesta una restricción acerca de su longitud \emph{a-priori}. Por contra, en este modelo no se permite conocer el suceso $A[i+1]$ en el momento $i$.  Esto encaja perfectamente en el modelo que se describirá en la sección \ref{sec:streaming_model}.

        \end{itemize}

        \paragraph{}
        Según la diferenciación que se acaba de indicar, estas dos estrategias de diseño de algoritmos encajan en disciplinas distintas, teniendo una gran ventaja a nivel de eficiencia en el caso estático los \emph{Algoritmos Offline}, pero quedando prácticamente in-utilizables cuando la computación es en tiempo real, donde es mucho más apropiado el uso de estrategias de diseño de \emph{Algoritmos Online}.

        \paragraph{}
        Como medida de eficiencia para los \emph{Algoritmos Online}, Karp \cite{Karp:1992:OAV:645569.659725} propone el \textbf{Ratio Competitivo}, el cual se define como la cota inferior del coste de cualquier nueva entrada con respecto de la que tiene menor coste. Sin embargo, dicha medida de eficiencia no es comúnmente utilizada en el caso de los \emph{Algoritmos para Streaming} por la componente estocástica de los mismos, para los cuales son más apropiadas medidas probabilistas. A continuación se describen las ventajas de estos respecto de su vertiente determinista.


      \subsection{Algoritmos Probabilistas}

        \paragraph{}
        Los \emph{Algoritmos Probabilistas} son una estrategia de diseño que emplea en un cierto grado de aleatoriedad en alguna parte de su lógica. Estos utilizan distribuciones uniformes de probabilidad para tratar de conseguir un incremento del rendimiento en su caso promedio. A continuación se describen los dos tipos de algoritmos probabilísticos según la clasificación realizada por Babai \cite{Babai79monte-carloalgorithms}:

        \begin{itemize}

          \item \textbf{Algoritmos Las Vegas}: Devuelven un resultado incorrecto con una determinada probabilidad, pero avisan del resultado incorrecto cuando esto sucede. Para contrarrestar este suceso basta con llevar a cabo una nueva ejecución del algoritmo, lo cual tras un número indeterminado de ejecuciones produce un resultado válido.

          \item \textbf{Algoritmos Monte Carlo}: Fallan con un cierto grado de probabilidad, pero en este caso no avisan del resultado incorrecto. Por lo tanto, lo único que se puede obtener al respecto es una indicador de la estimación del resultado correcto hacia la que converge tras varias ejecuciones. Además, se asegura una determinada cota del error $\epsilon$, que se cumple con probabilidad $\delta$.

        \end{itemize}

        \paragraph{}
        La razón anecdótica por la cual Babai \cite{Babai79monte-carloalgorithms} decidió denominar dichas categorías de algoritmos de esta manera se debe a lo siguiente (teniendo en cuenta el contexto de lengua inglesa): cuando se va a un casino en \emph{Las Vegas} y se realiza una apuesta el \emph{croupier} puede decir si se ha ganado o perdido porque habla el mismo idioma. Sin embargo, si sucede la misma situación en \emph{Monte Carlo}, tan solo se puede conocer una medida de probabilidad debido a que en este caso el \emph{croupier} no puede comunicarlo por la diferencia dialéctica.


      \subsection{Algoritmos Online Probabilistas vs Deterministas}

        \paragraph{}
        La idea subyacente acerca del diseño de los \emph{Algoritmos Online} es la mejora de eficiencia con respecto de sus homónimos estáticos cuando el conjunto de valores de entrada es dependiente de los resultados anteriores. Sin embargo, existen casos en que la frecuencia de ejecución del algoritmo, debido a una alta tasa de llegada de valores en la entrada, las soluciones deterministas se convierten en alternativas poco escalables.

        \paragraph{}
        Dicha problemática se ha incrementado de manera exponencial debido al avance tecnológico y la gran cantidad de información que se genera en la actualidad, que sigue creciendo a un ritmo desorbitado. Este fenómeno ha convertido en algo necesario el diseño de estrategias basadas en técnicas probabilísticas que reduzcan en gran medida el coste computacional que como consecuencia eliminan el determinismo de la solución.


    \section{Modelo en Streaming}
    \label{sec:streaming_model}

      \paragraph{}
      En esta sección se describen los aspectos formales del \emph{Modelo en Streaming}. Para ello se ha seguido la representación definida por Muthukrishnan \cite{Muthukrishnan:2005:DSA:1166409.1166410}. Lo primero por tanto, es definir un flujo de datos o \emph{Data Stream} como una \say{secuencia de señales digitalmente codificadas utilizadas para representar una transmisión de información} \cite{ITS-def-data-stream}. Muthukrishnan \cite{Muthukrishnan:2005:DSA:1166409.1166410} hace una aclaración sobre dicha definición y añade la objeción de que los datos de entrada deben tener un ritmo elevado de llegada. Debido a esta razón existe complejidad a tres niveles:

      \begin{itemize}

        \item \textbf{Transmisión}: Ya que debido a la alta tasa de llegada es necesario diseñar un sistema de interconexiones que permita que no se produzcan congestiones debido a la obtención de los datos de entrada.

        \item \textbf{Computación}: Puesto que la tarea de procesar la gran cantidad de información que llega por unidad de tiempo produce cuellos de botella en el cálculo de la solución por lo que es necesario implementar técnicas algorítmicas con un reducido nivel de complejidad computacional para contrarrestar dicha problemática.

        \item \textbf{Almacenamiento}: Debido a la gran cantidad de datos que se presentan en la entrada, deben existir técnicas que permitan almacenar dicha información de manera eficiente. Esto puede ser visto desde dos puntos de vista diferences: \begin{enumerate*}[label=\itshape\alph*\upshape)]
          \item tanto desde el punto de vista del espacio, tratando de minimizar el tamaño de los datos almacenados, maximizando la cantidad de información que se puede recuperar de ellos,
          \item como desde el punto de vista del tiempo necesario para realizar operaciones de búsqueda, adicción, eliminación o edición.
        \end{enumerate*}. Además, se debe prestar especial atención en la información que se almacena, tratando de reducirla al máximo prescindiendo de datos redundantes o irrelevantes.

      \end{itemize}

      \subsection{Formalismo para Streaming}
      \label{sec:streaming_formalism}

        \paragraph{}
        Una vez descritos los niveles de complejidad a los que es necesario hacer frente para abordar problemas en el \emph{Modelo en Streaming}, se realiza una descripción de los distintos modelos que propone Muthukrishnan \cite{Muthukrishnan:2005:DSA:1166409.1166410} en los apartados \ref{sec:streaming_time_series}, \ref{sec:streaming_cash_register} y \ref{sec:streaming_turnstile}. La especificación descrita en dichos apartados será seguida durante el resto del capítulo. Para ello nos basaremos en el siguiente formalismo:

        \paragraph{}
        Sea $a_1 ,a_2 ,... ,a_t,... $ un flujo de datos de entrada (\emph{Input Stream}), de tal manera que cada elemento debe presentar un orden de llegada secuencial respecto de $t \in \mathbb{M}$. Esto también se puede ver de la siguiente manera: el elemento siguiente a la llegada de $a_{t-1}$ debe ser $a_{t}$ y, por inducción, el próximo será $a_{t+1}$. Es necesario aclarar que $t$ no se refiere a unidades propiamente temporales, sino a la posición en la entrada.


        \begin{equation}
        \label{eq:streaming_A_function}
          \boldsymbol{A}_t:[1...N] \rightarrow \mathbb{R}^2
        \end{equation}


        \paragraph{}
        El siguiente paso para describir el formalismo es añadir la función $\boldsymbol{A}_t$, cuyo dominio e imagen se muestran en la ecuación \eqref{eq:streaming_A_function}. Esta función tiene distintas interpretaciones dependientes del \emph{Modelo en Streaming} bajo el cual se esté trabajando en cada caso, pero la idea subyacente puede resumirse asumiendo que la primera componente almacena el valor, mientras que la segunda almacena el número de ocurrencias de dicho valor. Algo común a todos ellos es la variable $t$, que se corresponde con resultado de la función en el instante de tiempo $t$. Por motivos de claridad, en los casos en que nos estemos refiriendo un único momento, dicha variable será obviada en la notación.

      \subsection{Modelo de Serie Temporal}
      \label{sec:streaming_time_series}

        \paragraph{}
        El \emph{Modelo de Serie Temporal} o \emph{Time Series Model} se refiere, tal y como indica su nombre, a una serie temporal, es decir, modeliza los valores que toma la variable $i$ respecto de $t$, codificados en el modelo como $a_t = (i,1)$. Nótese que se utiliza el valor $1$ en la segunda componente de $a_t$, la razón de ello se debe a la definición de la imagen de $\boldsymbol{A}$ en la ecuación \eqref{eq:streaming_A_function}. A pesar de ello, dicho campo es irrelevante en este modelo, por lo que se podría haber escogido cualquier otro arbitrariamente. La razón por la cual se ha utilizado el valor $1$ ha sido el refuerzo de la idea de que en este caso, el valor que toma $a_t$ en un determinado momento, no volverá a variar su valor, pero quedará obsoleto con la llegada de $a_{t+1}$.

        \paragraph{}
        El modelo se describe de manera matemática mediante la función $\boldsymbol{A}$, tal y como se ilustra en la ecuación \eqref{eq:streaming_time_series}. Textualmente, esto puede traducirse diciendo que la función $\boldsymbol{A}$ representa una estructura de datos que almacena el valor de todos los elementos recibidos en la entrada hasta el instante de tiempo $t$, es decir, actúa como un historial. Un ejemplo de este modelo son los valores en bolsa que toma una determinada empresa a lo largo del tiempo.

        \begin{equation}
  			\label{eq:streaming_time_series}
  				\boldsymbol{A}(t) = a_t
  			\end{equation}

      \subsection{Modelo de Caja Registradora}
      \label{sec:streaming_cash_register}

        \paragraph{}
        El \emph{Modelo de Caja Registradora} o \emph{Cash Register Model} consiste en la recepción de incrementos de un determinado valor $i$. El nombre del modelo hace referencia al funcionamiento de una caja registradora (suponiendo que el pago se realiza de manera exacta), que recibe billetes o monedas de tipos diferentes de manera secuencial.

        \paragraph{}
        Para describir dicho modelo, previamente hay que realizar una aclaración acerca del contenido del elemento $a_t = (i, I_t)$, de manera que $i$ representa el valor recibido, mientras que $I_t \geq 0$ indica el incremento en el instante $t$. Una vez aclarada esta definición, la función $\boldsymbol{A}_{t}$, se construye tal y como se indica en la ecuación \eqref{eq:streaming_cash_register}.

        \begin{equation}
  			\label{eq:streaming_cash_register}
  				\boldsymbol{A}_{t}(i) = {A}_{t-1}(i) + I_{t}
  			\end{equation}

        \paragraph{}
        El \emph{Modelo de Caja Registradora} es ampliamente utilizado en la formalización de problemas reales debido a que muchos fenómenos siguen esta estructura. Un ejemplo de ello es el conteo de accesos a un determinado sitio web, los cuales se corresponden con incrementos $I_t$, en este caso de carácter unitario realizados por un determinado usuario $i$ en el momento $t$.


      \subsection{Modelo de Molinete}
      \label{sec:streaming_turnstile}

        \paragraph{}
        El \emph{Modelo de Molinete} o \emph{Turnstile Model} se corresponde con el caso más general, en el cual no solo se permiten incrementos, sino que también se pueden realizar decrementos en la cuenta. El nombre que se le puso a este modelo se debe al funcionamiento de los molinetes que hay en las estaciones de metro para permitir el paso a los usuarios, que en la entrada incrementan la cuenta del número de personas, mientras que en la salida los decrementan. La relajación originada por la capacidad de decremento ofrece una mayor versatilidad, que permite la contextualización de un gran número de problemas en este modelo. Por contra, añade un numerosas complicaciones a nivel computacional, tal y como se verá a lo largo del capítulo.

        \paragraph{}
        Al igual que ocurre en el caso anterior, para describir este modelo, lo primero es pensar en la estructura de los elementos en la entrada, que están formados por $a_t = (i, U_t)$, algo muy semejante a lo descrito en el \emph{Modelo de Caja Registradora}. Sin embargo, en este caso $U_t$ no tiene restricciones en su imagen, sino que puede tomar cualquier valor tanto positivo como negativo, lo cual añade el concepto de decremento. La construcción de la función $\boldsymbol{A}_{t}$ se describe en la ecuación \eqref{eq:streaming_turnstile}.

        \begin{equation}
        \label{eq:streaming_turnstile}
          \boldsymbol{A}_{t}(i) = {A}_{t-1}(i) + U_{t}
        \end{equation}

        \paragraph{}
        Muthukrishnan \cite{Muthukrishnan:2005:DSA:1166409.1166410} hace una diferenciación dentro de este modelo dependiendo del nivel de exigencia que se le pide al modelo, se dice que es un \emph{Modelo de Molinete estricto} cuando se añade la restricción $\forall i, \forall t \ \boldsymbol{A}_{t}(i) \geq 0$, mientras que se dice que es un \emph{Modelo de Molinete relajado} cuando dicha restricción no se tiene en cuenta.

        \paragraph{}
        Un ejemplo de este modelo es el conteo del número de usuarios que están visitando un determinado sitio web, tomando $U_t$ el valor $1$ en el caso de una nueva conexión y $-1$ en el caso de de una desconexión. En este ejemplo el valor $i$ representa una determinada página dentro del sitio web.

    \section{Estructura básica}
    \label{sec:streaming_structure}

      \paragraph{}
      Puesto que la naturaleza intríseca de los \emph{Algoritmos para Streaming} hace que procesen los elementos de entrada según van llegando, esto presenta peculiaridades con respecto de otras categorías algorítmicas más asentadas y utilizadas en la actualidad. Por tanto, primero se describirá la estructura básica que siguen los algoritmos más comúnmente utilizados para después mostrar la estrategia seguida en el caso de Streaming.

      \paragraph{}
      Los algoritmos clásicamente estudiados para resolver la mayoría de problemas se basan la idea de funciones matemáticas. Es decir, se les presenta un conjunto de valores en la entrada, y a partir de ellos, realizan una determinada transformación sobre los datos, que genera como resultado una salida. Nótese que esta idea no impone ninguna restricción acerca de lo que puede suceder en dicho proceso, es decir, no se restringe el uso de estructuras de datos auxiliares o técnicas similares.

      \paragraph{}
      Esta visión no se enmarca correctamente en el contexto de los \emph{Algoritmos para Streaming}. La razón se debe a que la entrada no es propiamente un conjunto de datos, sino que se refiere a un flujo en sí mismo. Esta característica tiene como consecuencia que en un gran número de ocasiones ya no sea necesario obtener los resultados tras cada llamada al algoritmo, ya que estos podrían carecer de interés o requerir un sobrecoste innecesario. Por lo tanto, el concepto de función matemática pierde el sentido en este caso, ya que estas exigen la existencia de un valor como resultado.

      \paragraph{}
      Un concepto más acertado para modelizar un \emph{Algoritmo para Streaming} podría ser lo que en los lenguajes de programación derivados de \emph{Fortran} se denomina subrutina, es decir, una secuencia de instrucciones que realizan una tarea encapsulada como una unidad. Sin embargo, para poder describir correctamente la estructura de un \emph{Algoritmo para Streaming} hace falta algo más. La razón de ello es que a partir de dicho modelo de diseño no sería posible realizar peticiones sobre el resultado calculado, es decir, sería una estrategia puramente procedural. Para corregir dicha problemática surge el concepto de consulta o \emph{query}. A través de dicha idea se pretende representar la manera de obtener un resultado a partir del cómputo realizado hasta el momento actual.

      \paragraph{}
      En resumen, con dicha estrategia de diseño se consigue separar la parte de procesado de la entrada de la parte de consulta del resultado, lo cual proporciona amplias ventajas para el modelo seguido por los \emph{Algoritmos para Streaming}. Sin embargo, dicha estrategia produce un sobrecoste espacial con respecto del modelo de algoritmo clásico. Este se debe a la necesidad de mantener una estructura de datos en la cual se almacenen los resultados parciales referentes al flujo de entrada.

      \paragraph{}
      Los algoritmos \emph{Algoritmos para Streaming} se componen por tanto de algoritmo de procesamiento del flujo de datos, una estructura de datos que almacena dichos resultados, y por último, un algoritmo de procesamiento de la consulta o \emph{query} necesaria para obtener los resultados requeridos. a continuación se dividen las fases para el funcionamiento de un algoritmo de dichas características.

      \begin{itemize}

        \item \textbf{Inicialización}: En esta fase se llevan a cabo el conjunto de tareas necesarias para inicializar la estructura de datos que actuará como almacen de información durante el procesamiento del flujo de datos de entrada. Generalmente esto consite en el proceso de reservar memoria, inicializar a un valor por defecto la estructura de datos, etc. Sin embargo, existen técnicas más sofisticadas que requieren de una mayor carga computacional en esta fase.

        \item \textbf{Procesado}: Se corresponde con el procesamiento del flujo de datos de manera secuencial. La idea subyacente en esta fase es la de realizar una determinada operación sobre la estructura de datos y el elemento de entrada actual, de manera que se lleve a cabo una actualización sobre la misma. Nótese en que la manera en que se manipula dicha estructura de datos condiciona en gran medida el conjunto de peticiones que se podrán realizar sobre ella.

        \item \textbf{Consulta}: La fase de consulta se diferencia con respecto de la anterior por ser de carácter consultivo. Con esto nos estamos refiriendo a que dicha tarea no modifica el estado actual de la estructura de datos, sino que recoge información de la misma, que posiblemente transforma mediante alguna operación, para después obtener un valor como resultado de dicha petición.

      \end{itemize}

    \section{Medidas de Análisis y Conceptos Matemáticos}
    \label{sec:streaming_analysis}

      \paragraph{}
      Los \emph{Algoritmos para Streaming} se caracterizan por utilizar propiedades estadísticas en alguna parte (generalmente en el procesado) de su cómputo para obtener la solución con un menor coste computacional. En este caso, el coste que se pretende minimizar es el referido al espacio necesario para almacenar la estructura de datos auxiliar. Tal y como se ha dicho anteriormente, la razón de ello es debida a que se presupone un conjunto masivo de datos en la entrada, por lo que se pretende que el orden de complejidad espacial respecto de la misma sea de carácter sub-lineal ($o(N)$).

      \paragraph{}
      El objetivo es encontrar soluciones con un intervalo de error acotado que permitan llegar a la solución en un orden espacial de complejidad logarítmica ($O(log(N))$). Sin embargo, existen ocasiones en que no es posible llegar a una solución en dicho orden de complejidad, como es el caso de \emph{Algoritmos para Streaming} aplicados a problemas de \emph{Grafos}, en los cuales se relaja dicha restricción a un orden de complejidad \emph{poli-logarítmico} ($O(polylog(N))$).

      \paragraph{}
      El orden de complejidad \emph{poli-logarítmico} engloba el conjunto de funciones cuyo orden de complejidad presenta un crecimiento acorde a una función polinomial formada logaritmos. Matemáticamente esto se modeliza a través de la ecuación \eqref{eq:polylog-complexity}

      \begin{equation}
      \label{eq:polylog-complexity}
        a_{k}\log ^{k}(N)+\cdots +a_{1}\log(N)+a_{0} = O(polylog(N)) \in o(N).
      \end{equation}

      \paragraph{}
      En esta sección se muestran distintas estrategias para poder llevar a cabo la demostración de pertenencia a un determinado orden de complejidad de un \emph{Algoritmo para Streaming}. Debido a la elevada base estadística que requieren dichas demostraciones, a continuación se definen algunos conceptos básicos relacionadas con estimadores estadísticos, para después realizar una breve demostración acerca de distintas cotas de concentración de valores en una distribución en las sub-secciones \ref{sec:markov_inequality}, \ref{sec:chebyshev_inequality} y \ref{sec:chernoff_inequality}. Las definiciones que se exponen a continuación han sido extraídas de los apuntes del curso sobre \emph{Randomized Algorithms} \cite{aspnes2014notes} impartido por Aspnes en la \emph{Universidad de Yale} así como las de la asignatura de \emph{Estadística}\cite{estadistica2016notes} impartida en el Grado de Ingeniería Informática de la \emph{Universidad de Valladolid}.

      \subsection{Conceptos básicos de Estadística}
      \label{sec:basic_statistics}

        \paragraph{}
        Denotaremos como $x_1$ a una observación cualquiera contenida en el espacio de todas los posibles. Al conjunto de todas las observaciones posibles lo denotaremos como $\Omega$ y lo denominaremos espacio muestral, por lo tanto, $x \in \Omega$. Este concepto se puede entender de manera más sencilla mediante el siguiente ejemplo. Supongamos el lanzamiento de una moneda, que como resultado puede tomar los valores cara o cruz. Definiremos entonces $x_1 = \text{cara}$ y $x_2 = \text{cruz}$ como los sucesos posibles de lanzar una moneda. Por tanto el espacio $\Omega$ se define como $\Omega = \{x_1, x_2\} = \{\text{cara}, \text{cruz}\}$.

        \paragraph{}
        El siguiente paso es definir el concepto de \textbf{Variable Aleatoria}, que representa una función que mapea la realización de un determinado suceso sobre el espacio $\Omega$. Dicha función se denota con letras mayúsculas y puesto que sus parámetros de entrada son desconocidos, estos se ignoran en la notación. Por tanto denotaremos las variables aleatorias como ($\boldsymbol{E}, \boldsymbol{X}, \boldsymbol{Y}, \text{etc.}$). Para la variable aleatoria $\boldsymbol{X}$, sean $x_1, x_2, ..., x_i,...$ cada una de las observaciones posibles. Siguiendo el ejemplo anterior, se puede modelizar el lanzamiento de una moneda como $X$. Nótese por tanto, que una variable aleatoria puede definirse de manera textual como la modelización del resultado de un suceso \emph{a-priori} desconocido.

        \paragraph{}
        Definiremos probabilidad como la medida de certidumbre asociada a un suceso o evento futuro, expresada como un valor contenido en el intervalo $[0,1]$, tomando el valor $0$ un suceso imposible y $1$ un suceso seguro. La notación seguida para representar esto será $Pr[\boldsymbol{X} = x_i]$. Suponiendo la equi-probabilidad en el ejemplo de la moneda, podemos definir sus valores de probabilidad como $Pr[\boldsymbol{X} = \text{cara}] = \tfrac{1}{2}$ y $Pr[\boldsymbol{X} = \text{cruz}] = \tfrac{1}{2}$

        \paragraph{}
        Una vez descritos estos conceptos simples, a continuación hablaremos sobre distintos conceptos estadísticos utilizados en el análisis de algoritmos probabilísticos tales como \emph{Esperanza}, \emph{Varianza}, \emph{Variables Independientes} y \emph{Probabilidad Condicionada}.

        \paragraph{}
        Denominaremos \textbf{Esperanza Matemática} al valor medio o más probable que se espera que tome una determinada variable aleatoria. La modelización matemática de dicho concepto se muestra en la ecuación \eqref{eq:expectation}. Además, la esperanza matemática es de carácter lineal, por lo que se cumplen las ecuaciones \eqref{eq:expectation_l1} y \eqref{eq:expectation_l2}

        \begin{equation}
        \label{eq:expectation}
          \mathbb{E}[\boldsymbol{X}] = \sum_{i=1}^\infty x_i \cdot Pr[\boldsymbol{X} = x_i]
        \end{equation}

        \begin{equation}
        \label{eq:expectation_l1}
          \mathbb{E}[c \boldsymbol{X}] = c \mathbb{E}[\boldsymbol{X}]
        \end{equation}

        \begin{equation}
        \label{eq:expectation_l2}
          \mathbb{E}[\boldsymbol{X} + \boldsymbol{Y}] = \mathbb{E}[\boldsymbol{X}] + \mathbb{E}[\boldsymbol{Y}]
        \end{equation}

        \paragraph{}
        La \textbf{Varianza} se define como una medida de dispersión de una variable aleatoria. Dicho estimador representa el error cuadrático respecto de la esperanza. Su modelización matemática se muestra en la ecuación \eqref{eq:variance}. Aplicando propiedades algebraicas se puede demostrar la veracidad de las propiedades descritas en las ecuaciones \eqref{eq:variance_p1} y \eqref{eq:variance_p2}.

        \begin{equation}
        \label{eq:variance}
          Var[\boldsymbol{X}] = \mathbb{E}[(\boldsymbol{X} - \mathbb{E}[\boldsymbol{X}])^2]
        \end{equation}

        \begin{equation}
        \label{eq:variance_p1}
          Var[\boldsymbol{X}] = \mathbb{E}[\boldsymbol{X}^2] - \mathbb{E}^2[\boldsymbol{X}]
        \end{equation}

        \begin{equation}
        \label{eq:variance_p2}
          Var[c \boldsymbol{X}] = c^2 Var[\boldsymbol{X}]
        \end{equation}

        \paragraph{}
        A continuación se describe el concepto de \textbf{Independencia} entre dos variables aleatorias $\boldsymbol{X}, \boldsymbol{Y}$. Se dice que dos variables son independientes cuando los sucesos de cada una de ellas no están condicionados por los de otras. Esto puede verse a como el cumplimiento de la igualdad de la ecuación \eqref{eq:independence}.

        \begin{equation}
        \label{eq:independence}
          Pr[\boldsymbol{X} = x \cap \boldsymbol{Y} = y] = Pr[\boldsymbol{X} = x] \cdot Pr[\boldsymbol{Y} = y]
        \end{equation}

        \paragraph{}
        Cuando nos referimos al concepto de independencia referido a un conjunto $n$ variables aleatorias $\boldsymbol{X_1}, \boldsymbol{X_2},..., \boldsymbol{X_n}$ lo denominaremos \textbf{Independencia Mutua}, que impone la restricción descrita en la ecuación \eqref{eq:mutual_independence}.

        \begin{equation}
        \label{eq:mutual_independence}
          Pr \bigg[ \bigcap_{i=1}^n \boldsymbol{X_i} = x_i \bigg] = \prod_{i=1}^n Pr[\boldsymbol{X_i} = x_i]
        \end{equation}

        \paragraph{}
        También es de especial interés en el campo de los algoritmos probabilísticos el caso de la \textbf{k-independencia} sobre un conjunto de $n$ variables aleatorias $\boldsymbol{X_1}, \boldsymbol{X_2},..., \boldsymbol{X_n}$. Dicha idea se puede resumir como la independencia de todas las variables en grupos de $k$ variables. Este concepto tiene mucha importancia en el ámbito de los \emph{Sketches}, tal y como se verá en la sección \ref{sec:sketch}. El caso más simple es para $k = 2$, el cual se denomina \textbf{independencia pareada}, cuya modelización matemática se muestra en la ecuación \eqref{eq:pairwise_independence}.

        \begin{equation}
        \label{eq:pairwise_independence}
          \forall i, \forall j \ Pr[\boldsymbol{X_i} = x_i \cap \boldsymbol{X_j} = x_j] = Pr[\boldsymbol{X_i} = x_i] \cdot Pr[\boldsymbol{X_j} = x_j]
        \end{equation}

        \paragraph{}
        Desde el punto de vista de conjuntos de $n$ variables aleatorias  $\boldsymbol{X_1}, \boldsymbol{X_2},..., \boldsymbol{X_n}$, existen distintas propiedades de linealidad que se cumplen entre ellas a nivel del cálculo de la \emph{Esperanza} y la \emph{Varianza}. En el caso de la \emph{Esperanza}, la linealidad respecto de la suma (ecuación \eqref{eq:expectation_linearity}) se cumple para variables dependientes e independientes. Sin embargo, en el caso de la \emph{Varianza}, la linealidad respecto de la suma (ecuación \eqref{eq:variance_linearity}) se cumple tan solo para variables \textbf{independientes pareadas}.

        \begin{equation}
        \label{eq:expectation_linearity}
          \mathbb{E}\bigg[\sum_{i=1}^n \boldsymbol{X_i}\bigg] = \sum_{i=1}^n \mathbb{E}[\boldsymbol{X_i}]
        \end{equation}

        \begin{equation}
        \label{eq:variance_linearity}
          Var\bigg[\sum_{i=1}^n \boldsymbol{X_i}\bigg] = \sum_{i=1}^n Var[\boldsymbol{X_i}]
        \end{equation}

        \paragraph{}
        La \textbf{Probabilidad Condicionada} entre dos variables aleatorias $\boldsymbol{E_1}$ y $\boldsymbol{E_2}$ se puede definir como la medida de verosimilitud de la ocurrencia del suceso $\boldsymbol{E_1}$ sabiendo que ya ha ocurrido $\boldsymbol{E_2}$. Esto se puede modelizar matemáticamente tal y como se muestra en la ecuación \eqref{eq:conditional_probability}.

        \begin{equation}
        \label{eq:conditional_probability}
          Pr[\boldsymbol{E_1} \rvert \boldsymbol{E_2}] = \frac{Pr[\boldsymbol{E_1} \cap \boldsymbol{E_2}]}{Pr[\boldsymbol{E_1}]}
        \end{equation}

        \paragraph{}
        En el caso de la \textbf{Probabilidad Condicionada} sobre variables independientes, surge la propiedad descrita en la ecuación \eqref{eq:conditional_probability_independence}. Es fácil entender la razón, que se apoya en la idea de que si dos variables aleatorias no guardan relación, entonces la ocurrencia de una de ellas, no condicionará el resultado de la otra.


        \begin{equation}
        \label{eq:conditional_probability_independence}
          Pr[\boldsymbol{X_1} = x_1 \rvert \boldsymbol{X_2} = x_2] =
          \frac{Pr[\boldsymbol{X_1} = x_1 \cap  \boldsymbol{X_2} = x_2]}{Pr[\boldsymbol{X_1} = x_1]} =
          \frac{Pr[\boldsymbol{X_1} = x_1 \cdot \boldsymbol{X_2} = x_2]}{Pr[\boldsymbol{X_1} = x_1]} =
          Pr[\boldsymbol{X_2} = x_2]
        \end{equation}


        \paragraph{}
        Una vez descritos los conceptos estadísticos básicos para el análisis de algoritmos probabilísticos, lo siguiente es realizar una exposición acerca de las distintas cotas de concentración de valores, lo cual permite obtener resultados aproximados acerca de los resultados esperados por dichos algoritmos, así como sus niveles de complejidad. Primero se describirá \emph{Desigualdad de Boole}, para después tratar las desigualdades de \emph{Markov}(\ref{sec:markov_inequality}), \emph{Chebyshev}(\ref{sec:chebyshev_inequality}) y \emph{Chernoff}(\ref{sec:chernoff_inequality})

        \paragraph{}
        La \textbf{Desigualdad de Boole} consiste en una propiedad básica que indica que la probabilidad de que se cumpla la ocurrencia de un suceso es menor o igual que ocurrencia de la suma de todas ellas. Esto se modeliza matemáticamente en la ecuación \eqref{eq:boole_inequality}.

        \begin{equation}
        \label{eq:boole_inequality}
          Pr\bigg[\bigcup_{i=1}^n \boldsymbol{E_i}\bigg] \leq \sum_{i=1}^n Pr[\boldsymbol{E_i}]
        \end{equation}

      \subsection{Desigualdad de Markov}
      \label{sec:markov_inequality}

        \paragraph{}
        La \emph{Desigualdad de Markov} es la técnica base que utilizan otras desigualdades más sofisticadas para tratar de acotar la \emph{Esperanza} de una determinada \emph{Variable Aleatoria}. Proporciona una cota superior de probabilidad respecto de la \emph{Esperanza} tal y como se muestra en la ecuación \eqref{eq:markov_inequality}. Tal y como se puede intuir, dicha cota es muy poco ajustada, sin embargo, presenta una propiedad muy interesante como estructura base. Sea $f: \boldsymbol{X} \rightarrow \mathbb{R}^+$ una función positiva, entonces también se cumple la desigualdad de la ecuación \eqref{eq:markov_inequality_function}. El punto interesante surge cuando se escoge la función $f$ de tal manera que sea estrictamente creciente, entonces se cumple la propiedad descrita ecuación \eqref{eq:markov_inequality_function_positive}, a través de la cual podemos obtener cotas mucho más ajustadas. Dicha idea se apoya en la \emph{Desigualdad de Jensen}.

        \begin{equation}
        \label{eq:markov_inequality}
          \forall \lambda \geq 0, \ Pr[\boldsymbol{X} \geq \lambda ] \leq \frac{\mathbb{E}[\boldsymbol{X}]}{\lambda}
        \end{equation}

        \begin{equation}
        \label{eq:markov_inequality_function}
          \forall \lambda \geq 0, \ Pr[f(\boldsymbol{X}) \geq f(\lambda) ] \leq \frac{\mathbb{E}[f(\boldsymbol{X})]}{f(\lambda)}
        \end{equation}

        \begin{equation}
        \label{eq:markov_inequality_function_positive}
          \forall \lambda \geq 0, \ Pr[\boldsymbol{X} \geq \lambda ] = Pr[f(\boldsymbol{X}) \geq f(\lambda) ] \leq \frac{\mathbb{E}[f(\boldsymbol{X})]}{f(\lambda)}
        \end{equation}


      \subsection{Desigualdad de Chebyshev}
      \label{sec:chebyshev_inequality}

        \paragraph{}
        La \emph{Desigualdad de Chebyshev} utiliza la técnica descrita en la sub-sección anterior apoyándose en al idea de la función $f$ para obtener una cota de concentración mucho más ajustada basándose en la \emph{Varianza}. Dicha propiedad se muestra en la ecuación \eqref{eq:chebyshev_inequality}. En este caso se utiliza $f(\boldsymbol{X}) = \boldsymbol{X}^2$, que es estrictamente creciente en el dominio de aplicación de $\boldsymbol{X}$. Además, se selecciona como variable aleatoria $|\boldsymbol{X} - \mathbb{E}[\boldsymbol{X}]$, es decir, el error absoluto de una  $\boldsymbol{X}$ respecto de su valor esperado. La demostración de esta idea se muestra en la ecuación \eqref{eq:chebyshev_inequality_demo}.

        \begin{equation}
        \label{eq:chebyshev_inequality}
          \forall \lambda \geq 0, \ Pr[|\boldsymbol{X} - \mathbb{E}[\boldsymbol{X}]| \geq \lambda]  \leq \frac{Var[\boldsymbol{X}]}{\lambda^2}
        \end{equation}

        \begin{equation}
        \label{eq:chebyshev_inequality_demo}
          \forall \lambda \geq 0, \
          Pr[|\boldsymbol{X} - \mathbb{E}[\boldsymbol{X}]| \geq \lambda] =
          Pr[(\boldsymbol{X} - \mathbb{E}[\boldsymbol{X}])^2 \geq \lambda^2] \leq
          \frac{\mathbb{E}[(\boldsymbol{X} - \mathbb{E}[\boldsymbol{X}])^2]}{\lambda^2} =
          \frac{Var[\boldsymbol{X}]}{\lambda^2}
        \end{equation}

      \subsection{Desigualdad de Chernoff}
      \label{sec:chernoff_inequality}

        \paragraph{}
        En este apartado se realiza una descripción acerca de la \emph{Desigualdad de Chernoff}. Dicha descripción ha sido extraída de los apuntes de la asignatura de Algoritmos Probabilísticos (\emph{Randomized Algorithms}) \cite{chawla2004chernoff} impartida por Shuchi Chawla en la \emph{Carnegie Mellon University} de Pennsylvania.


        \paragraph{}
        La \emph{Desigualdad de Chernoff} proporciona cotas mucho más ajustadas que por contra, exigen unas presunciones más restrictivas para poder ser utilizada. La variable aleatoria en este caso debe ser de la forma $\boldsymbol{S} = \sum_{i=1}^n \boldsymbol{X_i}$ donde cada $\boldsymbol{X_i}$ es una variable aleatoria uniformemente distribuida e independiente del resto. También describiremos la esperanza de cada una de las variables $\boldsymbol{X_i}$ como $\mathbb{E}[\boldsymbol{X}] = p_i$.

        \paragraph{}
        Denotaremos como $\mu$ a la esperanza de $\boldsymbol{S}$, tal y como se describe en la ecuación \eqref{eq:chernoff_inequality_expectation}. También se define la función $f$ como $f(\boldsymbol{S}) = e^{t\boldsymbol{S}}$.

        \begin{equation}
        \label{eq:chernoff_inequality_expectation}
          \mu = \mathbb{E}\bigg[\sum_{i=1}^n \boldsymbol{X_i}\bigg] = \sum_{i=1}^n  \mathbb{E}[\boldsymbol{X_i}] = \sum_{i=1}^n p_i
        \end{equation}

        \paragraph{}
        El siguiente paso es utilizar la ecuación \eqref{eq:markov_inequality_function_positive} de la \emph{Desigualdad de Markov} con la función $f$, que en este caso es posible puesto que es estrictamente creciente. En este caso en lugar de utilizar $\lambda$ como constante, se prefiere $\delta \in [0,1]$, que se relacionada con la anterior de la siguiente manera: $\lambda = (1 + \delta)$. Entonces la ecuación \eqref{eq:chernoff_inequality_raw} muestra el paso inicial para llegar a la \emph{Desigualdad de Chernoff}.

        \begin{equation}
        \label{eq:chernoff_inequality_raw}
          Pr[ \boldsymbol{X} > (1+\delta) \mu] =
          Pr[ e^{t \mathbb{X}} > e^{(1+\delta)t\mu}] \leq
          \frac{\mathbb{E}[ e^{t \boldsymbol{X}}] }{e^{(1 + \delta) t \mu}}
        \end{equation}

        \paragraph{}
        Aplicando operaciones aritméticas y otras propiedades estadísticas, se puede demostrar la veracidad de las ecuaciones \eqref{eq:chernoff_inequality_upper} y \eqref{eq:chernoff_inequality_lower}, que proporcionan cotas mucho muy ajustadas de concentración de la distribución de una \emph{variable aleatoria} formada por la suma de $n$ variables aleatorias independientes uniformemente distribuidas.

        \begin{equation}
        \label{eq:chernoff_inequality_upper}
          \forall \delta \geq 0, \ Pr[\boldsymbol{X} \geq (1 + \delta)\mu]  \leq e^\frac{-\lambda^2\mu}{2 + \lambda}
        \end{equation}

        \begin{equation}
        \label{eq:chernoff_inequality_lower}
          \forall \delta \geq 0, \ Pr[\boldsymbol{X} \leq (1 - \delta)\mu]  \leq e^\frac{-\lambda^2\mu}{2 + \lambda}
        \end{equation}

      \subsection{Funciones Hash}
      \label{sec:hash_functions}

        \paragraph{}
        Las funciones hash son transformaciones matemáticas basadas en la idea de trasladar un valor en un espacio discreto con $n$ posibles valores a otro de $m$ valores de tamaño menor tratando de evitar que se produzcan colisiones en la imagen. Por tanto son funciones que tratan tratan de ser inyectivas en un sub-espacio de destino menor que el de partida. Sin embargo, tal y como se puede comprender de manera intuitiva es una propiedad imposible de cumplir debido a los tamaños del espacio de partida y de destino.

        \paragraph{}
        Las familias de funciones hash universales se refieren a distintas categorías en las cuales se pueden agrupar las funciones hash según el nivel de propiedades que cumplen. La categorías en las que se agrupan se denominan \emph{funciones hash k-universales}, de tal manera que el valor $k$ indican la dificultad de aparición de colisiones. Las funciones \emph{2-universales} se refieren a aquellas en las cuales se cumple que $Pr[h(x) = h(y)] \leq 1/m$ siendo $h$ la función hash, $x$ e $y$ dos posibles entradas tales que $x \neq y$ y $m$ el cardinal del conjunto de todas las posibles claves. Se dice que una función hash es \emph{fuertemente 2-universal} (\emph{strongly 2-universal}) si cumple que $Pr[h(x) = h(y)] \leq 1/m^2$ y genéricamente se dice que una función es \emph{k-universal} si cumple que $Pr[h(x) = h(y)] \leq 1/m^-k$. A continuación se describen dos estrategias básicas de diseño de funciones hash. Posteriormente se realiza una descripción acerca de las funciones hash sensibles a la localización.

        \subsubsection{Hash basado en Congruencias}
        \label{sec:hash_congruential}

          \paragraph{}
          Las funciones hash basadas en congruencias poseen la propiedad de ser \emph{2-universales}. Se basan en la idea de utilizar como espacio de destino aquel formado por $\mathbb{Z}_p$, es decir, todos los enteros que son congruentes con $p$ siendo $p \geq m$ un número primo. Además, se utilizan los enteros $a,b \in \mathbb{Z}_p$ con $a \neq 0$. La función hash entonces se describe tal y como se indica en la ecuación \eqref{eq:hash_congruential}.

          \begin{equation}
          \label{eq:hash_congruential}
            h_{ab}(x) = (ax + b) mod p
          \end{equation}

        \subsubsection{Hash basado en Tabulación}
        \label{sec:hasch_tabulation}

          \paragraph{}
          El hash basado en tabulación consiste en un método de generación de valores hash que restringe su dominio de entrada a cadenas de texto de tamaño fijo (u otras entradas que puedan codificarse de dicha forma). Denominaremos $c$ al tamaño fijo y  $T_i, i \in [1,c]$ a vectores que mapean el carácter $i$-ésimo de manera aleatoria. Entonces la función hash realiza una operación \emph{or-exclusiva} sobre los $T_i[x_i]$ valores tal y como se indica en la ecuación \eqref{eq:hash_tabulation}. Las funciones hash que se construyen siguiendo esta estrategia poseen la propiedad pertenecer a la categoría de las \emph{3-universales}.

          \begin{equation}
          \label{eq:hash_tabulation}
            h(x) = T_1[x_1] \oplus T_2[x_2] \oplus ... \oplus T_c[x_c]
          \end{equation}

      \subsubsection{Funciones Hash sensibles a la localización}
      \label{sec:hash_lsh}

        \paragraph{}
        Una categoría a destacar son las \emph{funciones hash sensibles a la localización}. Estas poseen la propiedad de distribuir los valores cercanos en el espacio de destino tratando mantener las propiedades de cercanía entre los valores. El primer artículo en que se habla de ellas es \emph{Approximate Nearest Neighbor: Towards Removing the Curse of Dimensionality} \cite{indyk1998approximate} de \emph{Indyk} y \emph{Motwani} inicialmente para resolver el problema de la \emph{búsqueda del vecino más cercano (Nearest Neighbor Search)}. Se trata de funciones hash multidimensionales, es decir, en la entrada están compuestas por más de un elemento. Estas funciones han cobrado especial importancia en los últimos años por su uso en problemas de dimensión muy elevada, ya que sirven como estrategia de reducción de la dimensionalidad del conjunto de datos, que como consecuencia reduce el coste computacional del problema.

        \paragraph{}
        \emph{Indyk} indica que para que una función hash sea sensible a la localización o $(r_1,r_2, p_1, p_2)$-sensible, para cualquier par de puntos de entrada $p,q$ y la función de distancia $d$ se cumpla la ecuación \eqref{eq:hash_lsh}. Las funciones hash que cumplen esta propiedad son interesantes cuando $p_1 > p_2$ y $r_1 < r_2$, de tal forma que para puntos cercanos en el espacio, el valor hash obtenido es el mismo, por lo que se asume que dichos puntos se encuentran cercanos en el espacio de partida.

        \begin{align}
        \label{eq:hash_lsh}
          \text{if} \ d(p,q) \leq r_1, \ & \text{then} \ Pr[h(x) = h(y)] \geq p_1 \\
          \text{if} \ d(p,q) \geq r_2, \ & \text{then} \ Pr[h(x) = h(y)] \leq p_2
        \end{align}

      \paragraph{}
      Una vez descritos los conceptos básicos acerca de lo que son los \emph{Algoritmos para Streaming} y las bases estadísticas necesarias para poder entender el funcionamiento de los mismo y sus niveles de complejidad así como de precisión, en las siguientes secciones se realiza una descripción sobre algunos de los algoritmos más relevantes en este área. En especial se explica el algoritmo de \emph{Morris} en la sección \ref{sec:streaming_morris_algorithm}, el de \emph{Flajolet-Martin} en la \ref{sec:streaming_morris_algorithm} y por último se hablará de la \emph{Estimación de Momentos de Frecuencia} en el modelo en streaming en la sección \ref{sec:streaming_frecuency_moment_aproximation}.


    \section{Algoritmo de Morris}
    \label{sec:streaming_morris_algorithm}

      \paragraph{}
      El \emph{Algoritmo de Morris} fue presentado por primera vez en el artículo \emph{Counting Large Numbers of Events in Small Registers} \cite{morris1978counting} redactado por \emph{Robert Morris}. En dicho documento se trata de encontrar una solución al problema de conteo de ocurrencias de un determinado suceso teniendo en cuenta las restriciones de espacio debido a la elevada tasa de ocurrencias  que se da en muchos fenómenos. El problema del conteo (\textbf{Count Problem}) de ocurrencias también se denomina el momento de frecuencia $F_1$ tal y como se verá en la sección \ref{sec:streaming_frecuency_moment_aproximation}.

      \paragraph{}
      Por tanto, \emph{Morris} propone realizar una estimación de dicha tasa para reducir el espacio necesario para almacenar el valor. Intuitivamente, a de partir dicha restricción se consigue un orden de complejidad espacial sub-lineal ($o(N)$) con respecto del número de ocurrencias. Se puede decir que el artículo publicado por \emph{Morris} marcó el punto de comienzo de este área de investigación. El conteo probabilista es algo trivial si se restringe a la condición de incrementar el conteo de ocurrencias siguiendo una distribución de \emph{Bernoulli} con un parámetro $p$ prefijado previamente. Con esto se consigue un error absoluto relativamente pequeño con respecto al valor $p$ escogido. Sin embargo, el error relativo que se obtiene cuando el número de ocurrencias es pequeño es muy elevado, lo cual lo convierte en una solución impracticable.

      \paragraph{}
      Para solucionar dicha problemática y conseguir una cota del error relativo reducida, la solución propuesta por \emph{Morris} se basa en la selección del parámetro $p$ variable con respecto del número de ocurrencias, con lo cual se consigue que la decisión de incrementar el contador sea muy probable en los primeros casos, lo cual elimina el problema del error relativo. \emph{Morris} propone aumentar el contador $X$ con probabilidad $\frac{1}{2^X}$. Tras $n$ ocurrencias, el resultado que devuelve dicho algoritmo es $\widetilde{n} = 2^X -1$. El pseudocódigo se muestra en el algoritmo \ref{code:morris-algorithm}.

      \paragraph{}
      \begin{algorithm}[h]
        \SetAlgoLined
        \KwResult{$\widetilde{n} = 2^X -1$ }
        $X \gets 0$\;
        \For{cada evento}{
          $X \gets X + 1 \ \text{con probabilidad} \frac{1}{2^X}$\;
        }
        \caption{Morris-Algorithm}
        \label{code:morris-algorithm}
      \end{algorithm}

      \paragraph{}
      A continuación se realiza un análisis de la solución. Esta ha sido extraída de los apuntes de la asignatura \emph{Algorithms for Big Data} \cite{bigdata2015jelani} impartida por \emph{Jelani Nelson} en la \emph{Universidad de Harvard}. Denotaremos por $X_n$ el valor del contador $X$ tras $n$ ocurrencias. Entonces se cumplen las igualdades descritas en las ecuaciones \eqref{eq:morris_expectation_1} y \eqref{eq:morris_expectation_2}. Esto se puede demostrar mediante técnicas inductivas sobre $n$.

      \begin{equation}
      \label{eq:morris_expectation_1}
        \mathbb{E}[2^{X_n}] = n + 1
      \end{equation}

      \begin{equation}
      \label{eq:morris_expectation_2}
        \mathbb{E}[2^{2X_n}] = \frac{3}{2}n^2 + \frac{3}{2}n + 1
      \end{equation}

      \paragraph{}
      Por la \emph{Desigualdad de Chebyshev} podemos acotar el error cometido tras $n$ repeticiones, dicha formulación se muestra en la ecuación \eqref{eq:morris_bound}.

      \begin{align}
      \label{eq:morris_bound}
        Pr[|\widetilde{n} - n| > \epsilon n ] < \frac{1}{\epsilon^2n^2}\cdot\mathbb{E}[\widetilde{n} - n]^2
          &= \frac{1}{\epsilon^2n^2}\cdot\mathbb{E}[2^X-1-n]^2
        \\&= \frac{1}{\epsilon^2n^2}\cdot \frac{n^2}{2}
        \\&= \frac{1}{2\epsilon^2}
      \end{align}

      \paragraph{}
      La ventaja de esta estrategia algorítmica con respecto de la trivial es la cota del error relativo producido en cada iteración del algoritmo, lo cual aporta una mayor genericidad debido a que esta se mantiene constante con respecto del número de ocurrencias. Sin embargo, se han propuesto otras soluciones para tratar de reducir en mayor medida dicha cota de error. El algoritmo \emph{Morris+} se basa en el mantenimiento de $s$ copias independientes de \emph{Morris} para después devolver la media del resultado de cada una de ellas. A partir de esta estrategia se consiguen las tasas de error que se indican en la ecuación \eqref{eq:morris+_bound}.

      \begin{align}
      \label{eq:morris+_bound}
        Pr[|\widetilde{n} - n| > \epsilon n ] < \frac{1}{2s\epsilon^2} && s > \frac{3}{2\epsilon^2}
      \end{align}


    \section{Algoritmo de Flajolet-Martin}
    \label{sec:streaming_flajolet_martin_algorithm}

      \paragraph{}
      En esta sección se describe el \emph{Algoritmo de Flajolet-Martin}, cuya descripción aparece en el artículo \emph{Probabilistic Counting Algorithms for Data Base Applications} \cite{flajolet1985probabilistic} redactado por \emph{Philippe Flajolet} y \emph{G. Nigel Martin}. En este caso, la problemática que se pretende resolver no es el número de ocurrencias de un determinado suceso, sino el número de sucesos distintos (\textbf{Count Distinct Problem}) en la entrada. Este problema también se conoce como el cálculo del momento de frecuencia $F_0$ tal y como se verá en la sección \ref{sec:streaming_frecuency_moment_aproximation}. Al igual que en el caso del algoritmo de \emph{Morris}, se apoya en estrategias probabilistas para ajustarse a un orden de complejidad espacial de carácter sub-lineal ($o(N))$) manteniendo una cota de error ajustada.

      \paragraph{}
      La intuición en la cual se basa el \emph{Algoritmo de Flajolet-Martin}, es la transformación de los elementos de entrada sobre una \emph{función Hash} universal binaria con distribución uniforme e independiente de probabilidad. La propiedad de distribución uniforme permite entonces prever que la mitad de los elementos tendrán un $1$ en el bit menos significativo, que una cuarta parte de los elementos tendrán un $1$ en el segundo bit menos significativo y así sucesivamente. Por tanto, a partir de esta idea se puede realizar una aproximación probabilista del número de elementos distintos que han sido presentados en la entrada. Requiere de $L$ bits de espacio para el almacenamiento del número de elementos distintos. Por la notación descrita en anteriores secciones $L = log(n)$, donde $n$ es el número máximo de elementos distintos en la entrada. A continuación se explica esta estrategia, para ello nos apoyaremos en las siguientes funciones:

      \begin{itemize}
        \item $hash(x)$ Es la función hash con distribución uniforme e independiente de probabilidad que mapea una entrada cualquiera a un valor entero en el rango $[0,...,2^L-1]$.
        \item $bit(y, k)$ Esta función devuelve el bit \emph{k-ésimo} de la representación binaria de $y$, de tal manera que se cumple que $y = \sum_{k \geq 0} bit(y,k)2^k$
        \item $\rho(y)$ La función $\rho$ devuelve la posición en la cual se encuentra el bit con valor $1$ empezando a contar a partir del menos significativo. Por convenio, devuelve el valor $L$ si $y$ no contiene ningún $1$ en su representación binaria, es decir, si $y = 0$. Esto se modeliza matemáticamente en la ecuación \eqref{eq:rho_function}.
      \end{itemize}

      \begin{equation}
      \label{eq:rho_function}
        \rho(y) =
          \begin{cases}
            min_{k \geq 0} bit(y, k) \neq 0 & y \geq 0\\
            L & y =0
          \end{cases}
      \end{equation}

      \paragraph{}
      \emph{Flajolet} y \emph{Martin} se apoyan en una estructura de datos indexada a la cual denominan \textbf{BITMAP}, de tamaño $[0...L-1]$ la cual almacena valores binarios $\{ 0, 1\}$ y se inicializa con todos los valores a $0$. Nótese por tanto, que esta estructura de datos puede ser codificada como un string binario de longitud $L$. La idea del algoritmo es marcar con un $1$ la posición \textbf{BITMAP[$\rho(hash(x))$]}. Seguidamente, queda definir el resultado de la consulta sobre cuántos elementos distintos han aparecido en el flujo de datos de entrada. Para ello se calcula $2^{\rho(\textbf{BITMAP})}$. El pseudocódigo se muestra en el algoritmo \ref{code:fm-algorithm}.

      \paragraph{}
      \begin{algorithm}[h]
        \SetAlgoLined
        \KwResult{$2^{\rho(\textbf{BITMAP})}$}
        \For{$i \in [0,...,L-1]$}{
          $\textbf{BITMAP[$i$]} \gets 0$\;
        }
        \For{cada evento}{
          \If{$\textbf{BITMAP[$\rho(hash(x))$]} = 0$}{
            $\textbf{BITMAP[$\rho(hash(x))$]} \gets 1$\;
          }
        }
        \caption{FM-Algorithm}
        \label{code:fm-algorithm}
      \end{algorithm}

      \paragraph{}
      El análisis de esta solución ha sido Extraído de los apuntes del libro \emph{Mining of massive datasets} \cite{leskovec2014mining} de la \emph{Universidad de Cambridge}. En este caso lo representa teniendo el cuenta el número de $0$'s seguidos en la parte menos significativa de la representación binaria de $h(x)$. Nótese que esto es equivalente al valor de la función $\rho(h(y))$, por tanto, adaptaremos dicho análisis a la solución inicial propuesta por \emph{Flajolet} y \emph{Martin}. La probabilidad de que se cumpla $\rho(h(x)) = r$ es $2^{-r}$. Supongamos que el número de elementos distintos en el stream es $m$. Entonces la probabilidad de que ninguno de ellos cumpla $\rho(h(x)) = r$ es al menos $(1- 2^{-r})^m$ lo cual puede ser reescrito como $((1- 2^{-r})^{2^r})^{m2^{-r}}$. Para valores suficientemente grandes $r$ se puede asumir que dicho valor es de la forma $(1-\epsilon)^{1/\epsilon} \approx 1/\epsilon$. Entonces la probabilidad de que no se cumpla que $\rho(h(x)) = r$ cuando han aparecido $m$ elementos distintos en el stream es de $e^{-m2^{-r}}$

      \paragraph{}
      La problemática de este algoritmo deriva de la suposición de la capacidad de generación de claves Hash totalmente aleatorias, lo cual no se ha conseguido en la actualidad. Por lo tanto posteriormente, \emph{Flajolet} ha seguido trabajando el problema de conteo de elementos distintos en \emph{Loglog counting of large cardinalities} \cite{durand2003loglog} y \emph{Hyperloglog: the analysis of a near-optimal cardinality estimation algorithm} \cite{flajolet2007hyperloglog} para tratar de mejorar el grado de precisión de su estrategia de conteo. En el artículo \emph{An optimal algorithm for the distinct elements problem} \cite{kane2010optimal} \emph{Daniel Kane y otros} muestran un algoritmo óptimo para el problema. Los resultados de dichos trabajos se discuten en la sección \ref{sec:hyper_log_log} por su cercana relación con las \emph{estructuras de datos de resumen}.


    \section{Aproximación a los Momentos de Frecuencia}
    \label{sec:streaming_frecuency_moment_aproximation}

      \paragraph{}
      La siguiente idea de la que es interesante hablar para terminar la introducción a los \emph{Algoritmos para Streaming} son los \emph{Momentos de Frecuencia}. Una generalización de los conceptos del número de elementos distintos ($F_0$) y el conteo de elementos ($F_1$) que se puede extender a cualquier $F_k$ para $k \geq 0 $. La definición matemática del momento de frecuencia $k$-ésimo se muestra en la ecuación \eqref{eq:frecuency_moments}. Nótese el caso especial de $F_\infty$ que se muestra en la ecuación \eqref{eq:frecuency_moments_max} y se corresponde con el elemento más veces común en el \emph{Stream}. Estas ideas han sido extraídas del documento \emph{Frequency Moments} \cite{woodruff2009frequency} redactado por \emph{David Woodruff}.

      \begin{equation}
      \label{eq:frecuency_moments}
        F_k = \sum_{i=1}^n m_i^k
      \end{equation}

      \begin{equation}
      \label{eq:frecuency_moments_max}
        F_\infty = max_{1 \leq i \leq n} m_i
      \end{equation}

      \paragraph{}
      El resto de la sección trata sobre la exposición de los algoritmos para el cálculo de los momentos de frecuencia descritos en el articulo \emph{The space complexity of approximating the frequency moments} \cite{alon1996space} redactado por \emph{Noga Alon}, \emph{Yossi Matias} y \emph{Mario Szegedy}, por el cual fueron galardonados con el premio \emph{Gödel} en el año 2005. En dicho trabajo además de presentar \emph{Algoritmos para Streaming} para el cálculo de $F_k$ (cabe destacar su solución para $F_2$), también presentan cotas inferiores para el problema de los \emph{Momentos de Frecuencia}. Posteriormente \emph{Piotr Indyk} y \emph{David Woodruff} encontraron un algoritmo óptimo para el problema de los\emph{Momentos de Frecuencia} tal y como exponen en \emph{Optimal Approximations of the Frequency Moments of Data Streams} \cite{indyk2005optimal}. A continuación se discuten los resultados de dichos trabajos.

      \paragraph{}
      Para el cálculo de $F_k$ para $k \geq 0$ \emph{Alon}, \emph{Matias} y \emph{Szedgedy} proponen un enfoque similar a los propuestos en algoritmos anteriores (la definición de una variable aleatoria $X$ tal que $\mathbb{E}[X] = F_k$). Sin embargo, la novedad en este caso es que su algoritmo no está restringido a un $k$ concreto, sino que en su caso es generalizable para cualquier entero positivo, sin embargo, en este caso la exposición es a nivel teórico.

      \paragraph{}
      Definiremos las constantes $S_1 =O(n^{1-1/k}/\lambda ^{2})$ y $S_2 = O(\log(1/\varepsilon ))$. El algoritmo utiliza $S_2$ variables aleatorias denominadas $Y_1, Y_2, Y_{S_2}$ y devuelve la mediana de estas denominándola $Y$. Cada una de estas variables $Y_i$ está formada por la media de $X_{ij}$ variables aleatorias tales que $1 leq j leq S_1$. La forma en que se actualiza el estado del algoritmo tras cada nueva llegada se apoya en el uso de $S_2$ funciones hash uniformemente distribuidas e independientes entre si que mapean cada símbolo a un determinado indice $j$.

      \paragraph{}
      Para el análisis del algoritmos supondremos que el tamaño $n$ del \emph{Stream} es conocido \emph{a-priori}. La demostración se apoya en una variable aleatoria $X$ construida de la siguiente manera:

      \begin{itemize}
        \item Seleccionaremos de manera aleatoria el elemento $a_{p \in (1,2,...,m)}$ del Stream, siendo $a_p = l \in (1,2,...n)$. Es decir, el elemento procesado en el momento $p$ representa la llegada del símbolo $l$.
        \item Definiremos $r=|\{q:q\geq p,a_{p}=l\}|$ como el número de ocurrencias del símbolo $l$ hasta el momento $p$.
        \item La variable aleatoria $X$ se define como $X=m(r^{k}-(r-1)^{k})$
      \end{itemize}

      \paragraph{}
      Desarrollando la Esperanza Matemática de la variable aleatoria $X$ se puede demostrar que esta tiende al momento de frecuencia $k$ tal y como se muestra en la ecuación \eqref{eq:expectation_frecuency_moments}.

      \begin{equation}
      \label{eq:expectation_frecuency_moments}
        {\displaystyle {\begin{array}{lll}\mathbb{E}(X)&=&\sum _{i=1}^{n}\sum _{i=1}^{m_{i}}(j^{k}-(j-1)^{k})\\&=&{\frac {m}{m}}[(1^{k}+(2^{k}-1^{k})+\ldots +(m_{1}^{k}-(m_{1}-1)^{k}))\\&&\;+\;(1^{k}+(2^{k}-1^{k})+\ldots +(m_{2}^{k}-(m_{2}-1)^{k}))+\ldots \\&&\;+\;(1^{k}+(2^{k}-1^{k})+\ldots +(m_{n}^{k}-(m_{n}-1)^{k}))]\\&=&\sum _{i=1}^{n}m_{i}^{k}=F_{k}\end{array}}}
      \end{equation}

      \paragraph{}
      En cuanto al coste espacial del algoritmo, se puede demostrar tal y como indican \emph{Alon}, \emph{Matias} y \emph{Szedgedy} en su artículo original \cite{alon1996space} que este sigue el orden descrito en la ecuación \eqref{eq:complexity_frecuency_moments} puesto que es necesario almacenar $a_p$ y $r$ lo cual requiere de $log(n) + log(m)$ bits de memoria, además de $S_1 x S_2$ variables aleatorias para mantener $X$.

      \begin{equation}
      \label{eq:complexity_frecuency_moments}
        {\displaystyle O\left({\dfrac {k\log {1 \over \varepsilon }}{\lambda ^{2}}}n^{1-{1 \over k}}\left(\log n+\log m\right)\right)}
      \end{equation}


  \section{Conclusiones}
  \label{sec:streaming_conclusions}

    \paragraph{}
    Tal y como se ha ilustrado a lo largo del capítulo, los \emph{algoritmos para streaming} son una solución adecuada tanto a nivel conceptual como práctica para problemas en los cuales el tamaño del conjunto de datos de entrada es tan elevado que no se puede hacer frente mediante estrategias clásicas. Por contra, estas soluciones presentan dificultades debido al elevado peso de la componente matemática y estadística que presentan. Además, la imprecisión en sus resultados restringe su uso en casos en los cuales la precisión es un requisito imprescindible por lo que tan solo deben ser utilizados en casos en los cuales no existan otras soluciones que calculen una solución con las restricciones de tiempo y espacio impuestas.

    \paragraph{}
    En este capítulo se ha realizado una introducción superficial acerca de este modelo, sin embargo las implementaciones y descripciones que se muestran tan solo gozan de importancia a nivel teórico y conceptual. Por lo tanto, en el capítulo \ref{chap:summaries} se continua la exposición de técnicas de tratamiento de grandes cantidades de datos desde una perspectiva más práctica hablando de las \emph{estructuras de datos de resumen}. Se describen en especial detalle las estructuras basadas en \emph{sketches}, que internamente utilizan \emph{algoritmos para streaming}.

\end{document}
